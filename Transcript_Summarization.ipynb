{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stop_words):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses the input text, removing stopwords and short tokens.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, deacc=True):\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_lists_from_pdf(file, num_topics, words_per_topic):\n",
    "    \"\"\"\n",
    "    Extracts topics and their associated words from a PDF document using the Latent Dirichlet Allocation (LDA) algorithm.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of num_topics sublists, each containing relevant words for a topic.\n",
    "    \"\"\"\n",
    "    loader = PyPDF2.PdfFileReader(file)\n",
    "    documents = [loader.getPage(i).extractText() for i in range(loader.numPages)]\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words(['english', 'spanish']))\n",
    "    processed_documents = [preprocess(doc, stop_words) for doc in documents]\n",
    "    dictionary = corpora.Dictionary(processed_documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    topics = lda_model.print_topics(num_words=words_per_topic)\n",
    "    topics_ls = []\n",
    "    for topic in topics:\n",
    "        words = topic[1].split(\"+\")\n",
    "        topic_words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n",
    "        topics_ls.append(topic_words)\n",
    "    return topics_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_from_pdf(llm, file, num_topics, words_per_topic):\n",
    "    \"\"\"\n",
    "    Generates descriptive prompts for LLM based on topic words extracted from a PDF document.\n",
    "\n",
    "    Returns:\n",
    "        str: A response generated by the language model based on the provided topic words.\n",
    "    \"\"\"\n",
    "    list_of_topicwords = get_topic_lists_from_pdf(file, num_topics, words_per_topic)\n",
    "    string_lda = \"\"\n",
    "    for lst in list_of_topicwords:\n",
    "        string_lda += str(lst) + \"\\n\"\n",
    "\n",
    "    template_string = '''Describe the topic of each of the {num_topics} double-quote delimited lists in a simple sentence and also write down three possible different subthemes. The lists are the result of an algorithm for topic discovery. Do not provide an introduction or a conclusion, only describe the topics. Do not mention the word \"topic\" when describing the topics. Use the following template for the response.\n",
    "\n",
    "    1: <<<(sentence describing the topic)>>>\n",
    "    - <<<(Phrase describing the first subtheme)>>>\n",
    "    - <<<(Phrase describing the second subtheme)>>>\n",
    "    - <<<(Phrase describing the third subtheme)>>>\n",
    "\n",
    "    2: <<<(sentence describing the topic)>>>\n",
    "    - <<<(Phrase describing the first subtheme)>>>\n",
    "    - <<<(Phrase describing the second subtheme)>>>\n",
    "    - <<<(Phrase describing the third subtheme)>>>\n",
    "\n",
    "    ...\n",
    "\n",
    "    n: <<<(sentence describing the topic)>>>\n",
    "    - <<<(Phrase describing the first subtheme)>>>\n",
    "    - <<<(Phrase describing the second subtheme)>>>\n",
    "    - <<<(Phrase describing the third subtheme)>>>\n",
    "\n",
    "    Lists: \"\"\"{string_lda}\"\"\" '''\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = chain.run({\n",
    "        \"string_lda\": string_lda,\n",
    "        \"num_topics\": num_topics\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogues_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extracts clinician-patient dialogues from a PDF file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing clinician and patient utterances.\n",
    "    \"\"\"\n",
    "    dialogues = []\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        for page_num in range(reader.numPages):\n",
    "            page_text = reader.getPage(page_num).extractText()\n",
    "            dialogue_chunks = re.split(r'Clinician: |Patient: ', page_text)\n",
    "            for i in range(1, len(dialogue_chunks), 2):\n",
    "                clinician_utterance = dialogue_chunks[i].strip()\n",
    "                patient_utterance = dialogue_chunks[i + 1].strip()\n",
    "                dialogues.append((\"Clinician\", clinician_utterance))\n",
    "                dialogues.append((\"Patient\", patient_utterance))\n",
    "    return dialogues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dialogues(llm, dialogues):\n",
    "    \"\"\"\n",
    "    Summarizes clinician-patient dialogues using a language model.\n",
    "\n",
    "    Returns:\n",
    "        str: A summarized version of the dialogues generated by the language model.\n",
    "    \"\"\"\n",
    "    dialogue_text = \"\\n\".join([f\"{speaker}: {utterance}\" for speaker, utterance in dialogues])\n",
    "    template_string = '''Summarize the clinician-patient dialogues provided below:\n",
    "\n",
    "    {dialogue_text}\n",
    "    '''\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = chain.run({\n",
    "        \"dialogue_text\": dialogue_text\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "openai_key = \"\"\n",
    "llm = OpenAI(openai_api_key=openai_key, max_tokens=-1)\n",
    "\n",
    "# Extract and summarize dialogues from PDF\n",
    "file_path = \"\"\n",
    "dialogues = extract_dialogues_from_pdf(file_path)\n",
    "summary = summarize_dialogues(llm, dialogues)\n",
    "print(summary)\n",
    "\n",
    "# Extract topics and generate responses\n",
    "num_topics = 6\n",
    "words_per_topic = 30\n",
    "topics_summary = topics_from_pdf(llm, file_path, num_topics, words_per_topic)\n",
    "print(topics_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
